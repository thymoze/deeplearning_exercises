{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - DL Tutorial 03\n",
    "\n",
    "Please complete the following notebook and submit your solutions to manuel.milling@informatik.uni-augsburg.de OR maurice.gerczuk@informatik.uni-augsburg.de by May 19, 23:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student name: \n",
    "    Benedikt Bauer\n",
    "    David Heim\n",
    "    Franz Schulze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mnist data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainx shape: (60000, 784)\n",
      "Trainy shape: (60000,)\n",
      "Testx shape:  (10000, 784)\n",
      "Testy shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "trainx, trainy, testx, testy = np.load('res/mnist.npy', allow_pickle=True)\n",
    "print(f\"Trainx shape: {trainx.shape}\")\n",
    "print(f\"Trainy shape: {trainy.shape}\")\n",
    "print(f\"Testx shape:  {testx.shape}\")\n",
    "print(f\"Testy shape:  {testy.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Implement sigmoid-function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X: np.array) -> np.array:  \n",
    "    \"\"\"\n",
    "    X: input: shape (num_samples, num_neurons)\n",
    "    return: element-wise application of sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   Implement forward propagation for one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcc_one_layer(H: np.array, W: np.array, b: np.array, activation):\n",
    "    \"\"\"\n",
    "    H: input: shape (num_samples, num_neurons_in)\n",
    "    W: weights: shape (num_neurons_in, num_neurons_out)\n",
    "    b: bias: shape (num_neurons_out,)\n",
    "    activation: activation function: python method\n",
    "    return: forward propagation of layer.\n",
    "    \"\"\"\n",
    "    return activation(b + H @ W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Implement the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    X: input: shape (num_samples, num_neurons)\n",
    "    return: softmax function applied to neuron axis.\n",
    "    \"\"\"\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.   Implement the neural network class with weight- and bias-initialization. Note: First initialise all weights, then all biases in order to compare your results for the given random seed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.   Implement the full forward propagation for our neural network and the given architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcc:\n",
    "    def __init__(self, n_input: int, n_hidden1: int, n_hidden2: int, n_out: int) -> None:\n",
    "        \"\"\"\n",
    "        n_input: number of neurons in input layer: int\n",
    "        n_hidden1: number of neurons in hidden layer 1: int\n",
    "        n_hidden2: number of neurons in hidden layer 2: int\n",
    "        n_out: number of neurons in output layer: int\n",
    "        \"\"\"\n",
    "        self.W = np.array([np.random.randn(n_input, n_hidden1), np.random.randn(n_hidden1, n_hidden2), np.random.randn(n_hidden2, n_out)], dtype=np.ndarray)\n",
    "\n",
    "        self.b = np.array([np.random.randn(n_hidden1), np.random.randn(n_hidden2), np.random.randn(n_out)], dtype=np.ndarray)\n",
    "\n",
    "    def forward_propagation(self, X: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        X: input: shape (num_samples, num_pixels)\n",
    "        return: predicition of the neural network\n",
    "        \"\"\"\n",
    "        H1 = fcc_one_layer(X, self.W[0], self.b[0], sigmoid)\n",
    "        H2 = fcc_one_layer(H1, self.W[1], self.b[1], sigmoid)\n",
    "        return fcc_one_layer(H2, self.W[2], self.b[2], softmax)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.   Implement a function for the cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(predictions, labels):\n",
    "    \"\"\"\n",
    "    predictions: predicted probabilities for classes: shape (num_samples, num_classes)\n",
    "    labels: correct classes: shape (num_samples,)\n",
    "    return: cross_entropy averaged across all samples \n",
    "    \"\"\"\n",
    "    return np.sum(-np.log(predictions[range(predictions.shape[0]), labels]) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.   Implement a function for the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels) -> float:\n",
    "    \"\"\"\n",
    "    predictions: predicted probabilities for classes: shape (num_samples, num_classes)\n",
    "    labels: correct classes: shape (num_samples,)\n",
    "    return: accuracy of the predictions\n",
    "    \"\"\"\n",
    "    return np.sum(np.argmax(predictions, axis=1) == labels) / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.   Evaluate the loss and the accuracy of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10196666666666666\n",
      "CrossEntropy: 13.444435797748726\n"
     ]
    }
   ],
   "source": [
    "NN = fcc(784, 400, 400, 10)\n",
    "pred = NN.forward_propagation(trainx)\n",
    "print(f\"Accuracy: {accuracy(pred, trainy)}\")\n",
    "print(f\"CrossEntropy: {cross_entropy(pred, trainy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.   Which are the parameters we can tune to improve the performance of our network? How many trainable parameters (scalars) does our network have in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and biases can be tuned to improve the performance. \n",
    "So we have four layers:\n",
    "- input layer with 784 neurons\n",
    "- first hidden layer with 400 neurons\n",
    "- second hidden layer with 400 neurons\n",
    "- out put layer with 10 neurons\n",
    "- \n",
    "and because this is a fully connected network each layer is fully connected to the next which means:\n",
    "\n",
    "> 784 x 400 + 400 x 400 + 400 x 10 = 477600 weights.\n",
    "\n",
    "But because the biases are also trainable we also have the biases of the two hidden layers and the output layer: \n",
    "\n",
    "> 400 + 400 + 10 = 810 biases\n",
    "\n",
    "Adding the weights: 810 + 477600 = 478410 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.  Why did we implement two different evaluation metrics of our system (cross-entropy and accuracy)? What are the main differences between the two and why can’t/shouldn’t we use them interchangeably?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is calculated by looking at the correct classifications and dividing them with all datapoints and its purpose is get a simple value to evaluate the performance of our network. But due to the way the accurracy is caclulated, it does not differentiate between the correct prediction of a particular value is 51% or 100%. \n",
    "The Cross-Entropy is a loss function and it takes the difference between a 100% and a 51% prediction into account because it is the average error of all datapoints and not a percentage value like the accuracy. So the target is to decrease the result of the loss function during training and use it to check if our network is progressing or is overshooting.\n",
    "But it is not really readable for humans and therefore we need the accuracy to interpret the performance of our network."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b28056c484a0cfa8de5ad0401556acbdbe329c33a6bce5eeee6dd07ed99920c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
