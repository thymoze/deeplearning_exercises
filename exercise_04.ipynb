{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - DL Tutorial 04: FCNN - BP \n",
    "\n",
    "Please complete the following notebook and submit your solutions to manuel.milling@informatik.uni-augsburg.de OR maurice.gerczuk@informatik.uni-augsburg.de till 26 May 23:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student name: \n",
    "> Benedikt Bauer, David Heim, Franz Schulze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions from exercise sheet 3 (class methods below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainx shape: (60000, 784)\n",
      "Trainy shape: (60000,)\n",
      "Testx shape:  (10000, 784)\n",
      "Testy shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#numpy random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "trainx, trainy, testx, testy = np.load('./res/mnist.npy', allow_pickle=True)\n",
    "print(\"Trainx shape: {}\".format(trainx.shape))\n",
    "print(\"Trainy shape: {}\".format(trainy.shape))\n",
    "print(\"Testx shape:  {}\".format(testx.shape))\n",
    "print(\"Testy shape:  {}\".format(testy.shape))\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1 +np.exp(-X))\n",
    "\n",
    "def softmax(X):\n",
    "    #more stable\n",
    "    eps = X.max()\n",
    "    return np.exp(X + eps)/(np.sum(np.exp(X + eps), axis=1).reshape((X.shape[0],1)))\n",
    "\n",
    "def fcc_one_layer(X, W, b, activation):\n",
    "    return activation(np.matmul(X, W) + b)\n",
    "\n",
    "def cross_entropy(pred_logits, y):\n",
    "    num_data_points = pred_logits.shape[0]\n",
    "    correct_logits = pred_logits[np.arange(num_data_points),y]\n",
    "    return np.mean(-np.log(correct_logits))\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    class_predictions = np.argmax(logits, axis=1)\n",
    "    return np.mean(class_predictions == labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Implement the error of the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_last_layer(H, y):\n",
    "    \"\"\"\n",
    "    :param H: softmax activations of shape (num_examples, num_classes)\n",
    "    :param y: correct labels of shape (num_examples,)\n",
    "    :return: delta of of last layer, i.e. derivative of cross entropy times derivative of softmax\n",
    "    \"\"\"\n",
    "    # note: this is destructive concerning H \n",
    "    # (which is not really a problem for the fcc class because H is not used after this, and you can regain H by running forward_propagation again)\n",
    "    H[range(H.shape[0]), y] -= 1 # using formula 32 \n",
    "    return H.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   Implement the derivative of the sigmoid function in terms of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_sigmoid(H: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param H: output of the sigmoid function shape (num_examples, num_units)\n",
    "    :return: element-wise derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return H * (1 - H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.   Implement the backpropagation as a class method.\n",
    "4.   Implement the the optimisation step as a class method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcc:\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_out):\n",
    "        #parameters\n",
    "        self.W_i_h1 = np.random.randn(n_input, n_hidden1)\n",
    "        self.W_h1_h2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "        self.W_h2_o = np.random.randn(n_hidden2, n_out)\n",
    "        self.b_h1 = np.random.randn(n_hidden1)\n",
    "        self.b_h2 = np.random.randn(n_hidden2)\n",
    "        self.b_out = np.random.randn(n_out)\n",
    "        #neuron activations and input H^n\n",
    "        self.X = None\n",
    "        self.h1 = None\n",
    "        self.h2 = None\n",
    "        self.out = None\n",
    "        #components of the gradient\n",
    "        self.dW_i_h1 = None\n",
    "        self.db_h1 = None\n",
    "        self.dW_h1_h2 = None\n",
    "        self.db_h2 = None\n",
    "        self.dW_h2_o = None\n",
    "        self.db_out = None\n",
    "\n",
    "        n_trainable_bias = self.b_h1.shape[0] + self.b_h2.shape[0] + self.b_out.shape[0]\n",
    "        n_trainable_weights = self.W_i_h1.shape[0] * self.W_i_h1.shape[1] + self.W_h1_h2.shape[0] * self.W_h1_h2.shape[1] + self.W_h2_o.shape[0] * self.W_h2_o.shape[1]\n",
    "        print(\"Number of parameters: {}\".format(n_trainable_bias + n_trainable_weights))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.X = X\n",
    "        self.h1 = fcc_one_layer(X, self.W_i_h1, self.b_h1, sigmoid)\n",
    "        self.h2 = fcc_one_layer(self.h1, self.W_h1_h2, self.b_h2, sigmoid)\n",
    "        self.out = fcc_one_layer(self.h2, self.W_h2_o, self.b_out, softmax)\n",
    "        return self.out\n",
    "\n",
    "    def backprop(self, y):\n",
    "        \"\"\"\n",
    "        :param y: labels, i.e. numbers of correct classes of shape (num_examples,)\n",
    "        \"\"\"\n",
    "        count_samples = self.X.shape[0]\n",
    "        delta_o = delta_last_layer(self.out, y)\n",
    "        self.dW_h2_o = (delta_o @ self.h2).T / count_samples # using formula 27\n",
    "        self.db_out = np.mean(delta_o, axis = 1) # using formula 28 \n",
    "        delta_h2 = (self.W_h2_o @ delta_o) * del_sigmoid(self.h2).T # using formula 26 and 33\n",
    "        self.dW_h1_h2 = (delta_h2 @ self.h1).T / count_samples # formula 27\n",
    "        self.db_h2 = np.mean(delta_h2, axis = 1) # using formula 28\n",
    "        delta_h1 = (self.W_h1_h2 @ delta_h2) * del_sigmoid(self.h1).T # using formula 26 and 33\n",
    "        self.dW_i_h1 = (delta_h1 @ self.X).T / count_samples # formula 27\n",
    "        self.db_h1 = np.mean(delta_h1, axis = 1) # using formula 28\n",
    "\n",
    "    def gradient_step(self, learning_rate):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning_rate for training\n",
    "        \"\"\"\n",
    "        self.W_i_h1 -= learning_rate * self.dW_i_h1\n",
    "        self.W_h1_h2 -= learning_rate * self.dW_h1_h2\n",
    "        self.W_h2_o -= learning_rate * self.dW_h2_o\n",
    "        self.b_h1 -= learning_rate * self.db_h1\n",
    "        self.b_h2 -= learning_rate * self.db_h2\n",
    "        self.b_out -= learning_rate * self.db_out\n",
    "\n",
    "    def train_single(self, X, y, learning_rate):\n",
    "        self.forward_propagation(X)\n",
    "        self.backprop(y)\n",
    "        self.gradient_step(learning_rate)\n",
    "\n",
    "    def train_mini_batch(self, X, y, learning_rate, batch_size):\n",
    "        for batchx, batchy in self.minibatches(X, y, batch_size):\n",
    "            self.train_single(batchx, batchy, learning_rate)\n",
    "\n",
    "    def minibatches(self, X: np.ndarray, y: np.ndarray, batch_size: int):\n",
    "        num_samples = X.shape[0]\n",
    "        all_indices = np.arange(num_samples)\n",
    "        # shuffle batches at every step\n",
    "        np.random.shuffle(all_indices)\n",
    "        for start_index in range(0, num_samples - batch_size + 1, batch_size):\n",
    "            batch_indices = all_indices[start_index : start_index + batch_size]\n",
    "            yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "    def write_data(self, trainx: np.ndarray, trainy: np.ndarray, testx: np.ndarray, testy: np.ndarray, iteration: int, file: str):\n",
    "        logits_train = self.forward_propagation(trainx)\n",
    "        logits_test = self.forward_propagation(testx)\n",
    "        f = open(file, \"a\")\n",
    "        f.write(f\"Epoch: {iteration}:\\n\")\n",
    "        f.write(f\"\\taccuracy: train_data: {accuracy(logits_train, trainy)}; test_data: {accuracy(logits_test, testy)}\\n\")\n",
    "        f.write(f\"\\tcross_entropy: train_data: {cross_entropy(logits_train, trainy)}; test_data: {cross_entropy(logits_test, testy)}\\n\")\n",
    "        f.close()\n",
    "\n",
    "    def train(self, trainx: np.ndarray, trainy: np.ndarray, testx: np.ndarray, testy: np.ndarray, learning_rate: int, iterations: int, write_frequency: int, output_file: str, batch_size = 0):\n",
    "        for i in range(iterations + 1):\n",
    "            if (i % write_frequency == 0):\n",
    "                self.write_data(trainx, trainy, testx, testy, i, output_file)\n",
    "            if batch_size > 0:\n",
    "                self.train_mini_batch(trainx, trainy, learning_rate, batch_size)\n",
    "            else: \n",
    "                self.train_single(trainx, trainy, learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.   Implement the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 478410\n",
      "Number of parameters: 478410\n",
      "Train Loss normal gradient descent:\t2.505757390249101\n",
      "Train Accuracy normal gradient descent:\t0.57275\n",
      "\n",
      "Train Loss mini batch gradient descent:\t0.16209650866632427\n",
      "Train Accuracy mini batch gradient descent:\t0.9557\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "num_iterations_mini_batch = 100\n",
    "\n",
    "# since we store network parameters as class variables we need for both variants a net\n",
    "neural_net = fcc(784, 400, 400, 10)\n",
    "neural_net_mini_batch = fcc(784, 400, 400, 10)\n",
    "\n",
    "# since we call train without batch_size, train(...) runs gradient decent without mini batches\n",
    "neural_net.train(trainx, trainy, testx, testy, learning_rate, num_iterations, write_frequency=100, output_file=\"normal.txt\")\n",
    "neural_net_mini_batch.train(trainx, trainy, testx, testy, learning_rate, num_iterations_mini_batch, write_frequency=10, output_file=\"mini_batch.txt\", batch_size=64)\n",
    "\n",
    "logits = neural_net.forward_propagation(trainx)\n",
    "logits_minibatch = neural_net_mini_batch.forward_propagation(trainx)\n",
    "\n",
    "print(f\"Train Loss normal gradient descent:\\t{cross_entropy(logits, trainy)}\")\n",
    "print(f\"Train Accuracy normal gradient descent:\\t{accuracy(logits, trainy)}\")\n",
    "\n",
    "print(f\"\\nTrain Loss mini batch gradient descent:\\t{cross_entropy(logits_minibatch, trainy)}\")\n",
    "print(f\"Train Accuracy mini batch gradient descent:\\t{accuracy(logits_minibatch, trainy)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
